%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12) 
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\input{../tools/common_beamer.tex}
\graphicspath{{../figures}}

%----------------------------------------------------------------------------------------
%	 TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Sttatistical Learning]{Statistical Learning} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Jordi Villà i Freixa} % Your name
\institute[FCTE] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Universitat de Vic - Universitat Central de Catalunya \\
Study Abroad\\ % Your institution for the title page
\medskip
\textit{jordi.villa@uvic.cat} % Your email address
}
%\date{\today} % Date, can be changed to a custom date
\date{course 2023-2024}
\logo{\includegraphics[width=.1\textwidth]{FCTE}}
\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Índex} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Preliminary note}
  The material in these slides is strongly based on \cite{kroese2020}. When other materials are used, they are cited accordingly.

  Mathematical notation follows as good as it can a [good practices proposal](https://ctan.math.utah.edu/ctan/tex-archive/macros/latex/contrib/mlmath/mlmath.pdf) from the Beijing Academy of Artificial Intelligence.
  \end{frame}

%------------------------------------------------
\section{Introduction} % Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
%------------------------------------------------

%\subsection{Subsection Example} % A subsection can be created just before a set of slides with a common theme to further break down your presentation into chunks


\begin{frame}
\frametitle{How is data analyzed and used?}


\begin{description}
  \item [Statistical learning] interpret the model and quantify the uncertainity of the data.
  \item [Machine learning] (or {\em data mining} making predictions using large scale data.
\end{description}

The goals of modelling data are:
\begin{itemize}
  \item to predict data, based on existing one;
  \item to discover unusual or interesting patterns in data. 
\end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Tools to model data}

\begin{description}
  \item [Function approximation] Model data with approximate and simple functions or maps.
  \item [Optimization] Given a set of feasible mathematical models to the data, we may need to find the optimal one by fitting or callibrating a function to observed data.
  \item [Probability and Statistics] Probability theory and statistical inference provides ways to quantify the uncertainity inherent in making predictions based on observed data.
\end{description}
\end{frame}

%-----------------------------------

\begin{frame}[allowframebreaks]
  \frametitle{Some basic notation}

  Given an input or {\em feature} vector $\bm{x}$, ML aims at predicting an ouput or {\em response} variable vector $\bm{y}$. In particular, we search for a mathematical {\em prediction function} $g$ such that we can {\em guess} an approximation to $\bm{y}$, $\hat{\bm{y}}$:
  \begin{align*}
      g \colon \mathcal{X}  & \rightarrow \mathcal{Y}\\
      \bm{x}                &\mapsto \hat{\bm{y}}=g(\bm{x})
  \end{align*}

  \begin{definition}
    Dataset  $S=\{\vz_i\}_{i=1}^n=\{(\vx_i,\vy_i)\}_{i=1}^n$ is sampled from a distribution $\fD$ over a domain $\fZ=\fX\times\fY$.\\
    $\fX$  is the instance domain (a set), $\fY$ is the label domain (a set), and $\fZ=\fX\times\fY$ is the example domain (a set).
  \end{definition}
  
  Usually,
  $\fX$ is a subset of $\sR^d$ and $\fY$ is a subset of $\sR^{d_{o}}$, where $d$ is the input dimension, $d_{o}$ is the output dimension.
  
  $n=\#S$ is the number of samples. Without specification, $S$ and $n$ are for the training set.

  \begin{itemize}
    \item In {\em regression} problems, $\bm{y}$ is a vector of real values.
    \item In {\em classification} problems, $\bm{y}$ values lie within a finite set of $c$ categories: $y\in\{0, 1, \ldots, c-1\}$.
  \end{itemize}

  \begin{definition}  
  A hypothesis space is denoted by $\fH$. A hypothesis function is denoted by $f_{\vtheta}(\vx)\in\fH$ or $f(\vx;\vtheta)\in\fH$ with $f_{\vtheta}:\fX\to\fY$.
  \end{definition}
  
  $\vtheta$  denotes the set of parameters of $f_{\vtheta}$.
  
  If there exists a target function, it is denoted by $f^*$or $f:\fX\to\fY$ satisfying $\vy_i=f^*(\vx_i)$ for $i=1,\ldots,n$.
  
  A loss function, denoted by $\ell:\fH\times\fZ\to\sR_+:=[0,+\infty)$, measures the difference (or error) between a predicted label and a true label, e.g., $L^2$ loss:
  \[
      \ell(f_{\vtheta},\vz)=\frac{1}{2}(f_{\vtheta}(\vx)-\vy)^2,
  \]
  where $\vz=(\vx,\vy)$. $\ell(f_{\vtheta},\vz)$ can also be written as
  \[
      \ell(f_{\vtheta}(\vx),\vy)
  \]
  for convenience.
  
  (In the case of a classification, $\ell(f_{\vtheta},\vy)=\mathbbm{1}\{y\neq \hat{\vy}\}$)

We will see other useful loss functions (\{em cross entropy\} or {\em hinge} loss functions) later in this course.

It is unlikely that a mathematical function $g \equiv \ f_{\vtheta}:\fX\to\fY$ would be able to make accurate predictions of all possible pairs $\fZ=\fX\times\fY$.

  So, we use a probabilistic approach here to mpirical risk or training loss for a set $S=\{(\vx_i,\vy_i)\}_{i=1}^{n}$ is denoted by  $\LS(\vtheta)$ or $L_{n}(\vtheta)$ or $R_{n}(\vtheta)$ or $R_{S}(\vtheta)$,
  \begin{equation}
      \LS(\vtheta) =\frac{1}{n}\sum_{i=1}^n\ell(f_{\vtheta}(\vx_i),\vy_i).
  \end{equation}
  
  The population risk or expected loss is denoted by $L_{\fD}(\vtheta)$ or $R_{\fD}(\vtheta)$
  \begin{equation}
      \LD(\vtheta) =\Exp_{\fD}\ell(f_{\vtheta}(\vx),\vy),
  \end{equation}
  where $\vz=(\vx,\vy)$ follows the distribution $\fD$.
   
  (In the case of a classification, we denote $\LD(g) \equiv \LD(\vtheta) = \Prob_{\fD} [f_{\vtheta}(\vx) \neq \vy]$ and we say that $g$ is a classifier.)

  Because we are interested in minimizing the risk in our prediction, we are looking for the best possible $g* \colon = \mathrm{argmin}_g \Exp_{\fD}\ell(f_{\vtheta}(\vx),\vy)$

  (In classification, we look for $g* (\vx) = \underset{y\in\{0,1,\ldots,c-1\}}{\mathrm{argmax}}  \Prob[Y=y \ X=x]$.)

  \begin{theorem}
    For the squared-error loss $\ell (y,\hat{y})=(y-\hat{y})^2$, the optimal prediction function $g*$ is equal to the conditional expectation of $Y$ given $\vX=\vx$.
  \end{theorem}
  which leads to write the random response $Y$ as:
  \[Y=g*(\vx)+\varepsilon(\vx)\]
  Note that such random deviation satisfies $\Exp \varepsilon(\vx)=0$
\end{frame}


%----------------------------------------------------------------------------------------
\section{Bibliography}
\bibliographystyle{plain}
\bibliography{DataSciencewithPython}

\end{document}
