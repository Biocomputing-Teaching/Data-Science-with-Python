%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12) 
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\input{../tools/common_beamer.tex}
\graphicspath{{../figures}}

%----------------------------------------------------------------------------------------
%	 TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Unsupervised learning]{Unsupervised learning} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Jordi Villà i Freixa} % Your name
\institute[FCTE] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Universitat de Vic - Universitat Central de Catalunya \\
Study Abroad\\ % Your institution for the title page
\medskip
\textit{jordi.villa@uvic.cat} % Your email address
}
%\date{\today} % Date, can be changed to a custom date
\date{course 2023-2024}
\logo{\includegraphics[width=.1\textwidth]{FCTE}}
\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Índex} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------
\section{Introduction and scope}
\begin{frame}
  \frametitle{Preliminary note}
  The material in these slides is strongly based on \cite{kroese2020}. When other materials are used, they are cited accordingly.

  Mathematical notation follows as good as it can a \href{https://ctan.math.utah.edu/ctan/tex-archive/macros/latex/contrib/mlmath/mlmath.pdf}{good practices proposal} from the Beijing Academy of Artificial Intelligence.
  \end{frame}

%------------------------------------------------
\section{Introduction} % Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
%------------------------------------------------

%\subsection{Subsection Example} % A subsection can be created just before a set of slides with a common theme to further break down your presentation into chunks

\begin{frame}{What to expect?}
  In this session we will discuss:
  \begin{itemize}
    \item Unsupervised learning
    \item The Expectation-Maximization Algorithm
    \item Principal component analysis
  \end{itemize}
\end{frame}

\section{Unsupervised learning}

\begin{frame}{Unsupervised learning}
    \begin{figure}
        \includegraphics[width=0.7\linewidth]{supervised_unsupervised}
        \includegraphics[width=0.3\linewidth]{2dbivariate}
        \caption{Supervised-unsupervised learning, and actual pdf behind the data}
        \label{fig:supunsup}
    \end{figure}
\end{frame}

\begin{frame}
    {\bf Unsupervised learning} is the process of inferring hidden patterns from historical data. We try here to find similarities, differences, patterns and structure in data by itself, without human intervention. Within this set of techniques, one can find many different real life applications:
    \begin{itemize}
        \item data exploration, preparation or visualization,
        \item customer segmentation,
        \item anomaly detection,
        \item target marketing campaigns,
        \item recommender systems, and many more.
    \end{itemize}
    We use them, in general, for clustering and association problems. We exect less accurate results. Some algorithms: K-Means, Gaussian Mixture models, Frequent Pattern (FP) Growth, Principal Component Analysis, Hierarchical Clustering Analysis, etc.
\end{frame}

\begin{frame}
    \begin{itemize}
        \item Unsupervised learning is useful for data analysis when we do not know what are we looking for in the data. finding similarities, creating groups, etc. for example, by categorizing elements by their activity (cells with respect to their RNA expression, customer segmentation, etc).
        \item Data does not need to be labelled, and such data is much easier and faster to obtain.
        \item We can now find unknown paterns and useful insights in data.
        \item Last, but equally important, it reduces human biass and error.
    \end{itemize}
\end{frame}



\begin{frame}{Generating data}
    \begin{Exercise}{Generate bivariate data}
        Generate data from a collection of three different bivariate distributions with arbitrary means and covariances.
    \end{Exercise}
\end{frame}


\begin{frame}{Joint probability distribution}
    \begin{figure}
        \includegraphics[width=0.6\linewidth]{Multivariate_normal_sample}
        \caption{\href{https://commons.wikimedia.org/wiki/File:Multivariate_normal_sample.svg}{Multivariate normal sample}}
    \end{figure}
\end{frame}

\section{MLE vs EM}
\begin{frame}{Maximum likelihood and Expectation-Maximization algorithm}
    We already saw how to use the K-means algorithm, based on the {\em distances} of the different data points. Now we will use another method that is based on assuming a given distribution of the data. 
    \\[10pt]
    Maximum likelihood estimation (MLE) is an approach to density estimation for a dataset by searching accross probability distributions and their parameters. However, it needs that {\em latent} (or hidden) variables not to exist in the training dataset. It requires the dataset to be complete, or fully observed: that all variables that are relevant to the problem are present.
    \\[10pt]
    The {\bf expectation-maximization (EM)} algorithm is an approach for performing MLE when latent variables are present. It is common to use it in estimating the density when data is missing, such as clustering in the {\bf Gaussian Mixture Model}.
\end{frame}

\begin{frame}{The EM algorithm}
    Let us assume a {\em mixture model}: unspecified combinadtion of multiple probability distribution functions. In our case, we will deal with the {\em Gaussian mixture model (GMM)}, where the parameters to estimate are the mean and the standard deviation for each Gaussian (normal) distribution in the mixture.
    \begin{enumerate}
        \item E-step: Estimate the missing variables in the dataset: the expected value for each latent variable.
        \item M-step: Maximize the parameters of the model in the presence of the data, using MLE.
    \end{enumerate}
\end{frame}

\begin{frame}{A one dimensional example for EM}
    \begin{figure}
        \includegraphics[width=0.7\linewidth]{hist}
        \caption{Adapted from \href{https://machinelearningmastery.com/expectation-maximization-em-algorithm/}{here}}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{A one dimensional example for EM}
    \footnotesize
    \begin{lstlisting}
from numpy import hstack
from numpy.random import normal
from sklearn.mixture import GaussianMixture
# generate a sample
X1 = normal(loc=20, scale=5, size=3000)
X2 = normal(loc=40, scale=5, size=7000)
X = hstack((X1, X2))
# plot the histogram
pyplot.hist(X, bins=50, density=True)
pyplot.show()
# reshape into a table with one column
X = X.reshape((len(X), 1))
# fit model
model = GaussianMixture(n_components=2, init_params='random')
model.fit(X)
yhat = model.predict(X) # predict latent values
print(yhat[:100])       # check latent value for first few points
print(yhat[-100:])      # check latent value for last few points
    \end{lstlisting}
    \normalsize
\end{frame}

\begin{frame}{The maths behind}
    \[\mu_k = \frac{\sum_{i=1}^n p_i^{(t)}(k)\uvec{x}_i}{\sum_{i=1}^n p_i^{(t)}(k)}\]

    \[\mathrm{Cov}_k=\frac{\sum_{i=1}^n p_i^{(t)}(k)(\uvec{x}_i-\mu_k)(\uvec{x}_i-\mu_k)^T}{\sum_{i=1}^n p_i^{(t)}(k)}\]

    where we estimate the pdf by using the previous iteration expectation and covariace:

    \[p_i^{(t)}(k) \varpropto w_k^{(t-1)} \phi_k(\uvec{x}_i | \mu_k^{(t-1)},\mathrm{Cov}_k^{(t-1)})\]
\end{frame}


\section{Principal Component Analysis}

\begin{frame}{\href{https://builtin.com/data-science/step-step-explanation-principal-component-analysis}{PCA}}
    \begin{enumerate}
        \item Get some data
        \item Substract the mean from each data dimension. Better standardize: $z=\frac{\uvec{x}-\bar{\uvec{x}}}{\sigma}$. Calculate the \href{https://datascienceplus.com/understanding-the-covariance-matrix/}{covariance matrix} $X X^T$.
        \item Diagonalize and obtain unit eigenvectors via SVD: $X X^T=U D^2 U$
        \item Choosing components and forming a feature vector 
        \item Recast the data along the chosen PC axes. $\uvec{x}_i^{\mathrm{PC}}=U_k U_k^T \uvec{x}_i$
    \end{enumerate}
    \begin{figure}
        \includegraphics[width=0.7\linewidth]{projection_intuition}
        \caption{Maximizing variance in principal component space is equivalent to minimizing least-squares reconstruction error. Taken from \href{http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/}{this post}}
    \end{figure}
\end{frame}

\begin{frame}{EX2}
    \begin{Exercise}{EX2. PCA example}
        Get the {\em Iris} dataset from the \href{https://archive.ics.uci.edu}{UCI repository}. 
        \begin{enumerate}
            \item Using the {\bf kdeplot} method of {\bf seaborn}, generate a figure for the kernel density plots of the variable {\em Petal.length} for each of the three species of {\em Iris sp.} in the dataset.
            \item Show a scatter plot of the variable Petal.length vs the variable Sepal.length and comment on the correlation you see. 
            \item In order to analyze the possible correlatoons between the different data in the file, perform a PCA and obtain the principal component matrix $U$ as well as the diagonal matrix $D^2$. What can you say in terms of the variance?
            \item Project the data on the first component and plot the kernel density estimate of the PCA-combined data. 
        \end{enumerate}
        See also \href{https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html\#iris---edgar-andersons-iris-data}{implementations in R} of hierarchical clustering in this famous dataset.
    \end{Exercise}
\end{frame}

\section{Clustering}

\subsection{Hierarchical clustering analysis}

\begin{frame}{Hierarchical clustering}
    \begin{figure}
        \includegraphics[width=\linewidth]{HCA}
        \caption{Left, a cluster hierarchy of 15 clusters. Right, the corresponding dendrogram\cite{kroese2020}.}
    \end{figure}
\end{frame}


\begin{frame}{Hierarchical clustering}
    In hierarchical cluster analysis (or HCA) one seeks to build a hierarchy of clusters with one of these two strategies:
    \begin{itemize}
        \item Agglomerative: "bottom-up" approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
        \item Divisive: "top-down" approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.
    \end{itemize}

Merges and splits are determined in a {\bf greedy} manner. Results are usually presented in a dendrogram. Any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances. Except for some specific cases, the algorithms cannot be guaranteed to find the optimum solution. 
\end{frame}

\begin{frame}{HCA flavours}
    \href{https://online.stat.psu.edu/stat555/node/86/}{Different types of HCA} can be done, depending on the way the distance between points (typically the Euclidean distance) and between clusters $I$ and $J$ (how the clusters are linked: {\em linkage criterion}) is computed:
    \begin{itemize}
        \item Single linkage: The closest distance between the clusters
        \[
            d_{\mathrm{min}} = \underset{i \in I, j \in J}{\mathrm{min}}
            \mathrm{dist} 
            (\uvec{x}_i,\uvec{x}_j) 
        \]
        \item Complete linkage: The farthest distance between the clusters
        \[
            d_{\mathrm{max}} = \underset{i \in I, j \in J}{\mathrm{max}}
            \mathrm{dist} 
            (\uvec{x}_i,\uvec{x}_j) 
        \]
        \item Group average: The mean distance between clusters
        \item Ward's minimal variance: distance computed as the additional amount of "variance" that would be introduced if two clusters would be merged.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Algorithm for HCA}
    \begin{algorithm}[H]
        \DontPrintSemicolon
          
        \KwInput{Distance function; linkage function $d$; number of clusters $K$.}
        \KwOutput{The labels set for the tree}
        Initialize the set of $n$ cluster identifiers $I$;
        Initialize the corresponding label sets $L_i$;
        Initialize a distance marix $D=[d_{ij}]$;

        \For{$k=n+1$ \KwTo $2n-K$}{
            Find $i$ and $j<i$ in $I$ such that $d_{ij}$ is minimal;
            Create a new label set $L_k$;
            Add the new identifier $k$ to $I$ and remove the old identifiers $i$ and $j$ from $I$;
            Update the distance matrix $D$ with respect to $i$ and $j$;
            }
        \Return{$L_i$}\;
        \caption{Greedy Agglomerative Clustering}
    \end{algorithm}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=\linewidth]{clustering}
        \caption{Overview of clustering methods from the \href{https://scikit-learn.org/stable/modules/clustering.html}{scikit site}}
    \end{figure}
\end{frame}

\subsection{DBSCAN}

\begin{frame}{Density-Based Clustering of Applications with Noise (DBSCAN)}
    \begin{figure}
        \includegraphics[width=0.7\linewidth]{DBSCAN}
        \caption{Schema of DBSCAN clustering for the distinction of hiugh to low density data clusters. Obtained from the \href{https://elutins.medium.com/dbscan-what-is-it-when-to-use-it-how-to-use-it-8bd506293818}{Medium web site}. See also a good explanation of the difference between HCA and DBSCAN \href{https://ryanwingate.com/intro-to-machine-learning/unsupervised/hierarchical-and-density-based-clustering/}{here}.}
    \end{figure}
\end{frame}


\begin{frame}{Exercise on clustering}
    \begin{Exercise}{Testing DBSCAN}
        Try to use HCA and DBSCAN on the {\em Iris} dataset. You can use \href{https://www.kaggle.com/code/raphaelekete/cluster-analysis-of-breast-cancer-data-set/notebook}{as a template} the work done on the \href{https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic}{Breast Cancer Wisconsin (Diagnostic) Data Set}. 
        Which one of the two methods can provide better clustering of the data? What oher methods would perform better to separate the three species?
    \end{Exercise}
\end{frame}


\section{Bibliography}
\bibliographystyle{unsrt}
\bibliography{DataSciencewithPython}
\end{document}