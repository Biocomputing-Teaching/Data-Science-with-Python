{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIT 9. Convolutional Neural Networks on the MNIST dataset\n",
    "\n",
    "This Unit includes an implementation of a CNN to calssify digits from the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). Code adapted from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    ":style: unsrt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start by importing:\n",
    "* the dataset *MNIST*\n",
    "* the **Sequential** function\n",
    "* the differential layers:\n",
    "  * **Conv2D**: creates the convolutional kernel to produce a set of output features\n",
    "  * **[MaxPooling2D](https://deepai.org/machine-learning-glossary-and-terms/max-pooling)**: downsamples the input by taking the maximum value of the kernel, reducing the dimensionality of the model\n",
    "  * **Flatten**: flattens the input tensor into a one dimensional vector. Useful when transitioning from a CNN to a fully connected CNN\n",
    "  * **Dense**: standard fully connected NN layer. Each neuron in the layer is connected to all neurons in the previous layer. \n",
    "* **to_categorical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us now load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display a collection of sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[3],cmap='gray')\n",
    "plt.show()\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(X_train[i],cmap='gray')\n",
    "    plt.title(y_train[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "* Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-normalization data: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "post-normalization data: [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333336\n",
      "  0.6862745  0.10196079 0.6509804  1.         0.96862745 0.49803922\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
      "  0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "  0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.19215687 0.93333334 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.9843137\n",
      "  0.3647059  0.32156864 0.32156864 0.21960784 0.15294118 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.07058824 0.85882354 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.7764706  0.7137255  0.96862745 0.94509804\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
      "  0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.05490196 0.00392157 0.6039216\n",
      "  0.99215686 0.3529412  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.54509807\n",
      "  0.99215686 0.74509805 0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04313726\n",
      "  0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.13725491 0.94509804 0.88235295 0.627451   0.42352942 0.00392157\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31764707 0.9411765  0.99215686 0.99215686 0.46666667\n",
      "  0.09803922 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
      "  0.5882353  0.10588235 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.0627451  0.3647059  0.9882353\n",
      "  0.99215686 0.73333335 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.9764706\n",
      "  0.99215686 0.9764706  0.2509804  0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
      "  0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.15294118 0.5803922  0.8980392  0.99215686 0.99215686 0.99215686\n",
      "  0.98039216 0.7137255  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09411765 0.44705883\n",
      "  0.8666667  0.99215686 0.99215686 0.99215686 0.99215686 0.7882353\n",
      "  0.30588236 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.07058824 0.67058825 0.85882354 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.7647059  0.3137255  0.03529412 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.21568628 0.6745098\n",
      "  0.8862745  0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
      "  0.52156866 0.04313726 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.53333336 0.99215686\n",
      "  0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('pre-normalization data:',X_train[0])\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_text = X_test.astype('float32')/255\n",
    "print('post-normalization data:',X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reshape the dataset to be 4D\n",
    "  * `batch_size`: the number of training examples processed\n",
    "  * `height`/`width`: height/width of each input image in pixels\n",
    "  * `channels`: number of color channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = np.expand_dims(X_train,axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [One hot encoding](https://blog.gopenai.com/one-hot-encoding-for-ml-with-tensorflow-and-keras-50f67ca9681a) using the `to_categorial` function. It converts the data into an integer. In this particular case, the best choice for this dataset would be to apply *sparse categorical crossentropy loss*— for the simple reason that we don’t have to apply one-hot encoding if we use that loss function. However, for the sake of training, we will apply here one hot encoding to use *categorical crossentropy loss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "[7 2 1 ... 4 5 6]\n",
      "(10000, 10)\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_test)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "print(y_test.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model architecture\n",
    "\n",
    "* 4 layers (2 [convolutional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and 2 Maxpooling)\n",
    "* 1 flattening layer\n",
    "* 1 fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# first layer with 32 filters and a filter size of (3,3)\n",
    "model.add(Conv2D(32,(3,3),activation='relu',input_shape=X_train.shape[1:]))\n",
    "# we reduce the size to half with the MaxPooling layer\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "# the third layer uses 64 filters\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "# convert the 2D map to a 1D vector\n",
    "model.add(Flatten())\n",
    "# fully connected layer with 10 output units, as the categories we have. Linear transformation followed by a softmax to finally select one category\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model. We will use the Adam optimizer (Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us have a look at the different kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'conv2d_8/kernel:0' shape=(3, 3, 1, 32) dtype=float32, numpy=\n",
      "array([[[[-0.01574765, -0.13739952,  0.09605604, -0.14007269,\n",
      "          -0.11063258, -0.0334749 , -0.13182156,  0.04118705,\n",
      "           0.04190844, -0.04971192, -0.01821082,  0.01682448,\n",
      "           0.11729492,  0.06104551, -0.00040214, -0.06948119,\n",
      "          -0.07693874, -0.13624488, -0.08235139,  0.04657412,\n",
      "          -0.10005322, -0.09740805, -0.11793293, -0.00376557,\n",
      "          -0.01710555,  0.04462343,  0.00366573, -0.13558634,\n",
      "          -0.05533396,  0.06440818,  0.07084665,  0.06021133]],\n",
      "\n",
      "        [[-0.0810877 ,  0.12828897, -0.02423517,  0.09222065,\n",
      "          -0.13356435, -0.08714673,  0.01058881,  0.0644317 ,\n",
      "           0.10757008,  0.06170106,  0.08356209,  0.04089691,\n",
      "          -0.0766554 , -0.08470193,  0.06632993, -0.02828214,\n",
      "           0.07270044, -0.04088695, -0.10013168,  0.04047962,\n",
      "           0.06619971, -0.07580294, -0.06101199,  0.0295279 ,\n",
      "           0.06000018,  0.02701868, -0.02076128, -0.06780203,\n",
      "          -0.06592027,  0.06929867, -0.13681287, -0.01183045]],\n",
      "\n",
      "        [[-0.05640887,  0.04071686, -0.03507787, -0.04943804,\n",
      "          -0.07369375, -0.02465104, -0.08321928, -0.0039946 ,\n",
      "           0.07152662, -0.11864501, -0.02709384,  0.01311617,\n",
      "           0.02188851, -0.06183074,  0.07210194, -0.0264061 ,\n",
      "          -0.04687545,  0.13394438,  0.03854157, -0.05508493,\n",
      "          -0.08557729, -0.12606414, -0.06194948, -0.11099243,\n",
      "          -0.12675263,  0.07752603, -0.02589898, -0.1118488 ,\n",
      "           0.0528951 , -0.06079379,  0.04021242,  0.03983067]]],\n",
      "\n",
      "\n",
      "       [[[ 0.02709275, -0.12430203,  0.14088245, -0.06002729,\n",
      "          -0.03998493,  0.06335157,  0.0008094 , -0.04145385,\n",
      "          -0.11445789,  0.03260963,  0.03823729,  0.07050407,\n",
      "           0.02778529,  0.13873132,  0.11265086,  0.01146202,\n",
      "          -0.13255668, -0.12656447, -0.01519193, -0.07558633,\n",
      "          -0.03071925, -0.11755706,  0.05284484,  0.00378948,\n",
      "           0.11630537,  0.10254605,  0.05794169,  0.06093107,\n",
      "          -0.12434645, -0.00866313, -0.09505471,  0.07908754]],\n",
      "\n",
      "        [[-0.06742256, -0.12104376,  0.00921969, -0.01846775,\n",
      "           0.01480317,  0.12487026,  0.02975756,  0.10235228,\n",
      "           0.1311499 , -0.01378916, -0.00568742, -0.08668735,\n",
      "          -0.05208091,  0.02809075,  0.01242682, -0.06844389,\n",
      "           0.06340969,  0.05113088,  0.13196178,  0.00911576,\n",
      "          -0.01147605, -0.091295  ,  0.10610348,  0.12061532,\n",
      "          -0.07020695,  0.09210576,  0.03634603,  0.01757899,\n",
      "          -0.13641621, -0.02296447,  0.02864589,  0.08553992]],\n",
      "\n",
      "        [[-0.04832664, -0.00047089, -0.08626833,  0.0302306 ,\n",
      "          -0.08994616,  0.03414133,  0.05723467, -0.13677599,\n",
      "           0.05928312,  0.01648764,  0.03739278, -0.13752256,\n",
      "           0.06718853, -0.05548029, -0.01442018, -0.03945886,\n",
      "          -0.13872282,  0.10913138, -0.05596352,  0.04787993,\n",
      "          -0.13785812,  0.0387786 ,  0.08723365,  0.13254099,\n",
      "           0.05808318, -0.09251766, -0.05678038, -0.08567002,\n",
      "          -0.03292295,  0.03829455,  0.02405578,  0.12049855]]],\n",
      "\n",
      "\n",
      "       [[[-0.12023653,  0.1059909 , -0.10159903, -0.04730568,\n",
      "           0.13876443,  0.09946544,  0.06684706, -0.10342535,\n",
      "           0.12118252,  0.1128139 , -0.06237429,  0.08732091,\n",
      "          -0.08713795,  0.08699434,  0.14181788, -0.09128132,\n",
      "           0.10104507, -0.10758801,  0.0620566 ,  0.03464097,\n",
      "           0.08930413, -0.02301364,  0.0896489 ,  0.13895522,\n",
      "           0.00566959,  0.05864832,  0.12532966, -0.07188264,\n",
      "           0.02766167, -0.04431506,  0.12539737, -0.09930825]],\n",
      "\n",
      "        [[ 0.02429469,  0.06265542,  0.05653717,  0.08183552,\n",
      "           0.12330757,  0.11595477, -0.11195809,  0.06298549,\n",
      "          -0.08224082, -0.10866491, -0.06982219,  0.10301133,\n",
      "          -0.0885599 , -0.07003266, -0.06296929,  0.03286251,\n",
      "           0.11750634, -0.0014555 ,  0.11677836, -0.05989266,\n",
      "           0.13988428, -0.12177362,  0.07553376, -0.10716333,\n",
      "          -0.01327029, -0.01070203,  0.11534251, -0.11261916,\n",
      "           0.05092715, -0.0178343 ,  0.08321793,  0.07968117]],\n",
      "\n",
      "        [[-0.04843901, -0.12240542,  0.05927645, -0.05073369,\n",
      "           0.00936203, -0.07838712, -0.1321031 ,  0.11282693,\n",
      "          -0.08429687,  0.12401806,  0.04970808,  0.03091787,\n",
      "          -0.04053404,  0.0099909 ,  0.00488546, -0.12393086,\n",
      "          -0.05506103, -0.10476096,  0.07131602,  0.05855039,\n",
      "           0.08206497,  0.1281582 ,  0.11905439,  0.10525347,\n",
      "           0.04747607,  0.03364247, -0.11538427, -0.01961691,\n",
      "          -0.00801605, -0.08420954, -0.14065666,  0.04426609]]]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'conv2d_9/kernel:0' shape=(3, 3, 32, 64) dtype=float32, numpy=\n",
      "array([[[[-0.03939559, -0.04898956, -0.04733002, ...,  0.06137987,\n",
      "          -0.02758062, -0.07097781],\n",
      "         [ 0.0005106 , -0.01010082, -0.04042826, ...,  0.0092679 ,\n",
      "           0.08325506,  0.02595141],\n",
      "         [-0.01874759,  0.01011857, -0.03353242, ..., -0.01147308,\n",
      "          -0.02292889, -0.02648815],\n",
      "         ...,\n",
      "         [ 0.01776999,  0.07450006, -0.06960448, ...,  0.05928681,\n",
      "          -0.01625504,  0.04947684],\n",
      "         [-0.08192657,  0.01364819,  0.01209757, ..., -0.07473787,\n",
      "           0.05716855, -0.07642802],\n",
      "         [ 0.02219293,  0.08061608, -0.03385268, ..., -0.0519345 ,\n",
      "          -0.06570479,  0.05245838]],\n",
      "\n",
      "        [[ 0.01080374, -0.00566081, -0.06364809, ..., -0.07024344,\n",
      "          -0.02999926,  0.0020557 ],\n",
      "         [ 0.00931241, -0.02359531,  0.07769575, ...,  0.06010044,\n",
      "          -0.04215314,  0.03628999],\n",
      "         [ 0.05655537,  0.03193233, -0.04396069, ..., -0.05061334,\n",
      "           0.08236144,  0.07572832],\n",
      "         ...,\n",
      "         [ 0.02910159, -0.03215834, -0.02312978, ...,  0.00920343,\n",
      "           0.07242683,  0.05008218],\n",
      "         [ 0.00612396,  0.01207838,  0.0633773 , ...,  0.05743162,\n",
      "           0.04789291, -0.02970066],\n",
      "         [ 0.03782153,  0.01141566,  0.02677306, ...,  0.01086471,\n",
      "           0.00877017,  0.04371538]],\n",
      "\n",
      "        [[-0.02618399,  0.08129288, -0.00425214, ..., -0.05641475,\n",
      "          -0.05808286, -0.07786559],\n",
      "         [-0.001647  , -0.02141279, -0.05463294, ...,  0.03967571,\n",
      "           0.06652684, -0.02052846],\n",
      "         [ 0.07924601,  0.0268778 ,  0.05015572, ..., -0.03428664,\n",
      "           0.02973881, -0.04002086],\n",
      "         ...,\n",
      "         [ 0.07406922, -0.05984193, -0.03055821, ..., -0.00793139,\n",
      "           0.05640594, -0.05119711],\n",
      "         [-0.08044933, -0.06484415, -0.07011352, ...,  0.03910951,\n",
      "          -0.00198913,  0.02127329],\n",
      "         [ 0.04069672, -0.04981301,  0.04894205, ..., -0.02824531,\n",
      "          -0.03088657,  0.08007135]]],\n",
      "\n",
      "\n",
      "       [[[ 0.01701947,  0.01905283, -0.01429149, ..., -0.07619079,\n",
      "           0.04899166,  0.02652726],\n",
      "         [ 0.03036318, -0.06590792, -0.02160851, ..., -0.01963816,\n",
      "          -0.01640296,  0.07316453],\n",
      "         [ 0.02722996,  0.00906066,  0.04620448, ..., -0.0491518 ,\n",
      "           0.04315034,  0.00668345],\n",
      "         ...,\n",
      "         [-0.04237648,  0.06421531,  0.08253058, ...,  0.06466021,\n",
      "           0.02171967,  0.07992921],\n",
      "         [ 0.0762851 , -0.01424678,  0.07062771, ..., -0.01103318,\n",
      "          -0.0645059 ,  0.056945  ],\n",
      "         [-0.06139998,  0.05704329,  0.03066244, ...,  0.03383287,\n",
      "          -0.01318467,  0.01322156]],\n",
      "\n",
      "        [[ 0.08267096, -0.05890767,  0.05876323, ..., -0.00808519,\n",
      "           0.04339544, -0.01348913],\n",
      "         [ 0.00230966,  0.07032258, -0.03657492, ..., -0.07820597,\n",
      "           0.05723331,  0.08221329],\n",
      "         [ 0.02277571, -0.03407478,  0.08142308, ...,  0.06132451,\n",
      "          -0.06489378,  0.03889227],\n",
      "         ...,\n",
      "         [-0.01449267, -0.00808433, -0.06465723, ...,  0.07408459,\n",
      "           0.02522681, -0.00498334],\n",
      "         [-0.06875195,  0.06519411,  0.01443753, ...,  0.03482199,\n",
      "          -0.03246425, -0.02794294],\n",
      "         [-0.06510153,  0.05665549, -0.05335492, ...,  0.03713057,\n",
      "           0.06244536, -0.07258259]],\n",
      "\n",
      "        [[ 0.02610693, -0.0089219 ,  0.05594442, ..., -0.07330376,\n",
      "          -0.04505078, -0.0339062 ],\n",
      "         [ 0.01799607, -0.01445923,  0.01517709, ...,  0.01179234,\n",
      "           0.01563875, -0.00042188],\n",
      "         [ 0.00298083, -0.04019239,  0.02825125, ...,  0.06964315,\n",
      "          -0.03473886, -0.04252261],\n",
      "         ...,\n",
      "         [-0.00933089, -0.04177503,  0.00927909, ...,  0.00999224,\n",
      "          -0.07696857,  0.03430533],\n",
      "         [ 0.07722267, -0.07726437,  0.01376317, ...,  0.03994721,\n",
      "          -0.04543016,  0.08204696],\n",
      "         [-0.07892796, -0.0027977 , -0.00459152, ..., -0.0055475 ,\n",
      "          -0.00378555,  0.00652804]]],\n",
      "\n",
      "\n",
      "       [[[ 0.05973782, -0.04295323,  0.0514377 , ..., -0.03413846,\n",
      "           0.07540844,  0.01220244],\n",
      "         [-0.01337919, -0.01421788, -0.02436763, ...,  0.08251638,\n",
      "          -0.01086309,  0.04496274],\n",
      "         [-0.05358986,  0.04521696, -0.021903  , ..., -0.03737841,\n",
      "          -0.03570807, -0.08019879],\n",
      "         ...,\n",
      "         [-0.0423786 , -0.04097982, -0.06917971, ..., -0.03336052,\n",
      "          -0.04787799,  0.05613432],\n",
      "         [ 0.05237392, -0.01490412,  0.01778251, ...,  0.0387103 ,\n",
      "          -0.01247593, -0.03318638],\n",
      "         [-0.07810801,  0.05976286, -0.02644239, ..., -0.02022137,\n",
      "          -0.02825411, -0.06508939]],\n",
      "\n",
      "        [[ 0.01295777,  0.0382515 ,  0.06356648, ...,  0.03666411,\n",
      "          -0.02702735,  0.04589244],\n",
      "         [ 0.04200915,  0.01163542,  0.07619519, ...,  0.00185072,\n",
      "           0.0762834 ,  0.04429271],\n",
      "         [-0.02859809,  0.00364587, -0.06322084, ...,  0.05917159,\n",
      "          -0.03763263,  0.06668075],\n",
      "         ...,\n",
      "         [-0.05255129, -0.04842421,  0.07432785, ...,  0.05231585,\n",
      "           0.03982908,  0.0759762 ],\n",
      "         [ 0.0284538 ,  0.05336478,  0.01198617, ..., -0.01163852,\n",
      "           0.06535871, -0.02566602],\n",
      "         [ 0.00374361, -0.03318014,  0.0266495 , ..., -0.01110604,\n",
      "          -0.05289086,  0.04632416]],\n",
      "\n",
      "        [[-0.02417817,  0.03022388, -0.06905532, ..., -0.07619369,\n",
      "          -0.06178679, -0.07969832],\n",
      "         [ 0.07003631, -0.01030844,  0.05656297, ..., -0.00542036,\n",
      "           0.01713425, -0.06447862],\n",
      "         [-0.06802773,  0.01747817, -0.05486669, ...,  0.06553669,\n",
      "          -0.01212772,  0.01571057],\n",
      "         ...,\n",
      "         [-0.07450581,  0.03993386,  0.02566049, ..., -0.07819855,\n",
      "          -0.00477549, -0.06928843],\n",
      "         [ 0.02780881,  0.00071345, -0.0224098 , ...,  0.01436001,\n",
      "          -0.06330401,  0.04204492],\n",
      "         [-0.05498495, -0.03605449, -0.03781971, ..., -0.01337602,\n",
      "          -0.00421944,  0.02450579]]]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].kernel)\n",
    "print(model.layers[2].kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "* batch_size:\n",
    "* epochs: number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 45s 95ms/step - loss: 0.2787 - accuracy: 0.9219 - val_loss: 8.7201 - val_accuracy: 0.9772\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 50s 106ms/step - loss: 0.0725 - accuracy: 0.9786 - val_loss: 7.3173 - val_accuracy: 0.9824\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 48s 102ms/step - loss: 0.0532 - accuracy: 0.9839 - val_loss: 6.8643 - val_accuracy: 0.9855\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 49s 104ms/step - loss: 0.0446 - accuracy: 0.9862 - val_loss: 6.5098 - val_accuracy: 0.9847\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 49s 104ms/step - loss: 0.0359 - accuracy: 0.9892 - val_loss: 6.9052 - val_accuracy: 0.9855\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 49s 104ms/step - loss: 0.0314 - accuracy: 0.9905 - val_loss: 7.7434 - val_accuracy: 0.9848\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 49s 103ms/step - loss: 0.0276 - accuracy: 0.9916 - val_loss: 6.2430 - val_accuracy: 0.9868\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 49s 105ms/step - loss: 0.0241 - accuracy: 0.9925 - val_loss: 7.7178 - val_accuracy: 0.9859\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 48s 102ms/step - loss: 0.0208 - accuracy: 0.9933 - val_loss: 8.1691 - val_accuracy: 0.9863\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 50s 106ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 8.3964 - val_accuracy: 0.9877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8819781e10>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=128, epochs=10, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 8.3964 - accuracy: 0.9877 - 1s/epoch - 4ms/step\n",
      "Test accuracy: 0.9876999855041504\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,y_test,verbose=2)\n",
    "print('Test accuracy:',test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
